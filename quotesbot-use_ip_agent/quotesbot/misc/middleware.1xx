import logging 
import urllib.request
from bs4 import BeautifulSoup
import threading
from .proxy_tool import proxy_tool
import random
logger=logging.getLogger(__name__)

class CustomHttpProxyMiddleware(object):
    proxy_url_list=[]
    lock=threading.Lock()

    def __init__(self):
        logger.info(self.__class__.__name__+"初始化工作开始...")
        if len(self.__class__.proxy_url_list) == 0:
            if self.__class__.lock.acquire():
                if len(self.__class__.proxy_url_list) == 0 :
                    self.__class__.get_proxy_url_list()
                    logger.info(str(self.__class__.proxy_url_list))
                self.__class__.lock.release()
        logger.info(self.__class__.__name__+"初始化工作完成...")
    def process_request(self, request, spider):
        # TODO implement complex proxy providing algorithm
        if self.can_use_proxy(request):
            #proxy_url=random.choice(self.__class__.proxy_url_list)["proxy_url"]
            #proxy_url=self.__class__.proxy_url_list[0]["proxy_url"]
            proxy_url="http://114.249.117.143:9000"
            #logger.info("请求前的url" + proxy_url)
            try:
                request.meta['proxy'] = proxy_url
            except Exception as e:
                logging.critical("Exception %s" % e)
    #def process_response(self ,request, response, spider):
        #pass
    #def process_exception(self,request, exception, spider):
        #pass
    def can_use_proxy(self, request):
        """
        如果能使用请求，返回True,否则返回False,
        你可以在这里对request的属性进行判断，比如说request.meta["depth"]
        """
        return True



    @classmethod
    def get_proxy_url_list(cls):
        tool=proxy_tool()
        for i in range(1,4):
            tool.get_proxy_list()
            cls.proxy_url_list=tool.proxy_list
            if cls.proxy_url_list is None:
                continue
            if len(cls.proxy_url_list) >0 :
                break

                
