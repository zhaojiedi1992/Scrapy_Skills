
import logging 
from urllib.request import urlopen
import json
import time
from twisted.python.failure import Failure

logger=logging.getLogger(__name__)

class CustomHttpProxyMiddleware(object):
    current_proxy_url_used_count=0
    current_proxy_url=None

    def __init__(self):
        logger.info(u"进入初始化工作")
        #type(self).get_proxy_url()

    @staticmethod
    def get_proxy_info_one(cls):
        url="http://webapi.http.zhimacangku.com/getip?num=1&type=2&pro=0&city=0&yys=0&port=11&time=1&ts=1&ys=1&cs=1&lb=1&sb=0&pb=4&mr=1&regions="
        result=None
        try:
            json_str=urlopen(url).read()
            result=json.loads(json_str)
            logging.info(json_str)
        except Exception as e :
            logging.critical("Exception %s" % e)
        return result

    @staticmethod    
    def get_proxy_info(cls):
        max_count=4
        cur_count=0
        while True:
            result = cls.get_proxy_info_one()
            cur_count+=1
            if result["code"] ==0  or cur_count==max_count :
                return result
            time.sleep(6)

    @staticmethod
    def get_proxy_url(cls):
        url=None
        try:
            proxy_info=cls.get_proxy_info()
            url="http://" + proxy_info["data"][0]["ip"]+":" + str(proxy_info["data"][0]["port"])
        except Exception as identifier:
            pass
        cls.current_proxy_url_used_count=0
        cls.current_proxy_url=url
        logger.info("得到ip" + url)

    def process_request(self, request, spider):
        if type(self).get_proxy_url is not None:
            type(self).get_proxy_info()

        logger.info("当前的ip"+str(type(self).current_proxy_url_used_count) +str(type(self).current_proxy_url))
        # TODO implement complex proxy providing algorithm
        if self.use_proxy(request):
            proxy_url=type(self).current_proxy_url
            logger.info("请求前的url" + proxy_url)
            if proxy_url is None:
               proxy_url="http://wwwwwwwwwwwwwwwwwwwwwwwwwwwwwww" 
            #logging.info(proxy_url)
            try:
                request.meta['proxy'] = proxy_url
                type(cls).current_proxy_url_used_count+=1
            except Exception as e:
                logging.critical("Exception %s" % e)

    def process_response(self ,request, response, spider):
        logger.info("中间件" + str(response.status)+str(type(self).current_proxy_url) +","+ str(type(self).current_proxy_url_used_count))
        if response.status == 403:
            meta = request.meta
            if type(self).current_proxy_url_used_count<=20 : 
                meta["proxy"]=type(self).current_proxy_url
            else:
                meta["proxy"]=type(self).get_proxy_url()
            return request.replace(meta=meta,dont_filter=True)
        else:
             return response
       

    def use_proxy(self, request):
        """
        using direct download for depth <= 2
        using proxy with probability 0.3
        """
        #if "depth" in request.meta and int(request.meta['depth']) <= 2:
        #    return False
        #i = random.randint(1, 10)
        #return i <= 2
        return True